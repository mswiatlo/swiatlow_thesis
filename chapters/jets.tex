%!TEX root = ../swiatlow_thesis.tex
\label{chapter:jets-and-substructure}
\section{The Goal of Jets}

The goal of particle physics experiments--- as Chapter~\ref{chapter:detector} will describe--- is to measure the outgoing particles produced in collisions in order to reconstruct the short lived intermediate states which describe the fundamental processes of nature. Final state particles such as electrons and muons and photons are measured through their interactions with a detector, and their 4-vectors are used to reconstruct the event and the interesting particles produced in the collision. 

Sections~\ref{chapter:sm:qcd:freedom} and \ref{chapter:sm:qcd:confinement} seem to throw a wrench into this program: quarks and gluons, two types of commonly produced particles, cannot be measured directly because of their interactions with the Strong Nuclear Force and the process of \textit{confinement}. Confinement is the process which hides color charge from the world: colored particles always form color neutral pairs and triplets, and in the process create a shower of associated color neutral particles. Thus what interacts with a detector is not just one particle, like in the case of an electron or a muon, but instead a large spray of hadrons which originate from the original parton.

Is it possible to reconstruct the 4-vectors of quarks and gluons? What information is lost in the showering process? Can the showering process itself tell us something about the physics of the collision? The answer to these questions is what we look for when we study \textit{jets}. This is the whimsical, though certainly appropriate, name for the collimated sprays of particles produced by quarks and gluons as they shower and hadronize. 

Because strongly interacting particles are so commonly produced in LHC collisions, understanding jets is integral to being able to reconstruct events. This chapter addresses the theoretical issues of jet reconstruction, describing first how 4-vectors corresponding to QCD partons are constructed and then describing some aspects of the emerging field of \textit{jet substructure}, in which the shapes and structur of jets can be used to infer new information about events.



\section{Jet Algorithms}

At first glance, the process of jet identification should be trivial. Figure~\ref{fig:jets:dijet}, for example, has two clearly identified showers of particles on either side of the detector: we can easily identify these groupings by eye, sum up 4-momenta of the calorimeter cells therein, and then analyze our di-jet event.


%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{dijet.png}
\label{fig:jets:dijet}
\caption{An example dijet event recorded in the ATLAS detector.}
\end{figure}

%%%%%%%%%%%%%%%% 

But what happens when events become more complicated? How many jets are there in Figure~\ref{fig:jets:4-jet}? Or in the event displays in Figure~\ref{fig:jets:many-jet}? As the complexity of the event increases, it becomes increasingly difficult to tell the differences between jets simply by eye. Moreover, while this kind of ad-hoc identification is possible when discussing handfuls of events, the LHC detectors record 500 events per second, or even more, so our by-eye approach is not going to scale to the task at hand. 

%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{4-jet.png}
\label{fig:jets:4-jet}
\caption{An example 4-jet event recorded in the ATLAS detector.}
\end{figure}

%%%%%%%%%%%%%%%% 


%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{6-jet.png}
\includegraphics[width=0.45\textwidth]{8-jet.png}
\label{fig:jets:many-jet}
\caption{An example 6- and 8-jet event recorded in the ATLAS detector: as the complexity of events increases, the difficulty of cleanly identifying jets becomes more difficult.}
\end{figure}

%%%%%%%%%%%%%%%% 


The introduction of \textit{jet algorithms} solves this problem~\cite{Jetography}. Jet algorithms provide a detailed prescription for how to combine different detector objects (or simulation-level particles) into jets, and are used to automate the process of jet finding over the billions of events that the colliders produce.

What features connect or distinguish jet algorithms? A simple example is how they handle one completely isolated particle: any reasonable algorithm should identify that one particle as a jet. But what about two close-by particles, in the case, for example, that a quark radiated a gluon? Usually the algorithm has some distance parameter, typically denoted $R$, which specifies a distance scale over which particles should be combined or not. One other question is how these two particles are combined: should their 4-vectors be added, or just their energies, etc? The combination of the jet algorithm, the distance parameter, and the recombination scheme specify a \textit{jet definition}\cite{Jetography}. A jet algorithm should be able to run on a parton-level event from simulation, a hadron-level event from simulation after a power shower and hadronization, or on detector level objects (calorimeter energy measurements, tracks, etc). In all cases the algorithm takes in 4-vectors and returns 4-vectors.

What does it mean to be a ``good'' jet algorithm? This question has been at the heart of hadronic analyses for decades, but historically there was often little agreement on what metrics should be used to evaluate algorithms, with an especially large disconnect between the needs of theorists (focused on calculability) and experimentalists (focused on speed and ease of use). The Snowmass Accords of 1990 was the first effort to define a set of criteria jet algorithms should try to achieve~\cite{Huth}:
%
\begin{itemize}
\item Simple to implement in an experimental analysis;
\item Simple to implement in the theoretical calculation;
\item Defined at any order of perturbation theory;
\item Yields finite cross section at any order of perturbation theory;
\item Yields a cross section that is relatively insensitive to hadronization
\end{itemize}
%
In practice, it took twenty years for experiments to adopt something compatible.

\subsection{Cone Algorithms} 

In the meantime, experiments tended to use a class of algorithms referred to as \textit{cone algorithms}~\cite{Jetography}. Typically, the algorithms would follow the general steps of:
%
\begin{enumerate}
\item Find a seed, typically the highest energy object
\item Draw a cone of size $R = \sqrt{\Delta \eta^2 + \Delta \phi^2}$ around the cluster: join all objects inside the cone with the seed
\item Remove this object: it is a finished jet
\item Proceed with the next highest seed, returning to step 1
\item When all seeds are used up, run a secondary split/merge step on closeby jets to resolve ambiguities
\end{enumerate}
%
Improvements such as the iterative cone,  overlapping cone, or midpoint cone algorithms modify the details, usually by adding additional refinement stages, where the center of the jet is allowed to drift away from the seed.

The main advantage of the cone algorithms was their speed: determining jets was simply a matter of computing distances to the seed, and so scaled as $O(N)$ with the number of objects to be clustered, and so certainly the first point of the Snowmass Accord was fulfilled. Unfortunately, cone algorithms have a much more difficult time with the remaining points.

\subsection{IRC Safety}

The last four parts of the Snowmass Accords detail the robustness of a jet clustering algorithm: how sensitive is the clustering to various fluctuations, and how reliable is it when used to make theory calculations and predictions? The assessment of these issues, somewhat poorly defined by Snowmass but much better understood now, lies in the definitions of \textit{infrared} and \textit{colinear} safety, often referred to together as IRC safety~\cite{Jetography}. Infrared safety is the guarantee that the result of a jet algorithm does not change if there is additional very soft radiation in the event. A low energy gluon emission is possible at any point in the showering of a jet, but is such a small effect that the algorithm--- which is trying to tell us about the initiating parton--- should not be affected by it. Colinear safety, on the other hand, is the guarantee that if a one hard particle were to split into two nearly colinear softer particles, that the algorithm would return a consistent result. Once again, the exact evolution of the parton shower should not matter: the same energy is heading in more or less the same direction, and our algorithm should be robust to the number of particles this energy comes in.

%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{ir.pdf}
\label{fig:jets:ir}
\caption{An example of a colinear splitting which changes the result of a jet algorithm. The $x$-axis indicates angular position; the $y$-axis indicates the energy of a particle. Configurations a, b, c, d all have the same total energy, but while b creates one jet, d creates two. Figure from \cite{Jetography}.}
\end{figure}

%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{c.pdf}
\label{fig:jets:c}
\caption{An example of an infrared (i.e. very soft) emission which changes the result of a jet algorithm. In this case, a softly emitted gluon in c causes extra radiation which creates an extra seed between the two separated jets of a and b; this extra seed can, in some iterative cone algorithms, cause the merging of these two jets. Figure from \cite{Jetography}.}
\end{figure}

%%%%%%%%%%%%%%%% 

The weakness of cone algorithms to colinear effects is easiest to understand: all cone algorithms use seeds of various sorts, and the splitting of one hard particle can cause the two softer particles to be separately measured, and therefore fall under threshold. This is demonstrated schematically in Figure~\ref{fig:jets:ir}, where particle configuration d results in two jets in the collinear unsafe algorithm, even though the distribution of energy is nearly identical to that of configuration a.

Infrared safety is a more subtle effect, and depends on the details of the cone algorithm's iteration or split-merging steps. For example, in Figure~\ref{fig:jets:c}, the extra soft emmission between two otherwise well separated jets can cause a new soft seed to appear between the jets: the merging step of many cone algorithms looks precisely for additional radiation between jets to decide if a pair should be merged, and this can cause the number of jets in the event to change.

What, exactly, are the consequences for failing to respect these requirements? The obvious pitfall is sensitivity to the exact development of the parton shower, which point 5 of the Snowmass Accords also requires us to avoid. IRC safety is also closely related to the middle three points of the Snowmass Accords, which all deal with calculations of jet properties (cross-sections, \pt spectra, and so on). In particular, soft emmissions and colinear splittings (the IR and the C) in perturbative calculations are actually divergent and therefore blow up the calculation. The saving grace of the calculation is that these terms come in with opposite signs, and therefore cancel--- but if the jet algorithm separates these infinities into separate objects, they will not properly cancel~\cite{Jetography}. For these reasons, both non-perturbative parton shower simulations and perturbative theoretical calculations are automatically of limited use in comparisons to data: if we want to test our understanding of QCD in various regimes, we need jet algorithms which respect these sensitivities.

Note that the process of measuring an event always softens the effects of IRC unsafety: limited angular resolution can mean that a colinear split is measured together still, or measurement thresholds can prevent infrared radiation from occuring and spoiling the merging process. All of these effects are by definition very detector specific, however: the robustness of a theoretical calculation should not have to depend on the angular size of the readout of a calorimeter.

What exactly cause this sickness of the cone algorithms? The main issues are at the start and at the end of the clustering: the seed and the split/merge process. If we can remove seeds from the process, we can prevent the colinear safety issues; if we define the algorithm without drawing an explicit cone, we can remove much of the ambiguities that require the split/merging step.

\subsection{Sequential Recombination Algorithms}

Sequential recombination algorithms do exactly both of these things. The idea behind them is to focus not on seeds and the cone around them, but instead on the relationships between closeby particles. In particular, there is always a \textit{distance metric} between pairs of particles, and in every iteration of the algorithm, the pair with the minimum distance is \textit{combined} if their distance is below some threshold. The process continues until no more merges are possible~\cite{Jetography}.

This strategy clearly improves the situation with seeds: there is never an energy dependent ``primary'' particle used to create a jet. The distance metrics can be weighted by energy themselves to ensure that jets follow energy flow throughout the event, but the iterative merging process removes the pathological dependencies on single particles which plagued cone algorithms. The merging process also has a clear stopping point defined: this lets the boundaries of jets grow naturally, without the concerns of overlaps that a simple cone drawing adds.

%origins

The first sequential algorithms we used in $e^+/e^-$ collisions: these were natural targets, as the ``clean'' leptonic collisions do not produce the additional underlying event contamination and large jet multiplicity inherent to hadronic collisions. Particle multiplicity is always a concern when analyzing events with sequential recombination algorithms, as the minimized distance calculation typically goes as $O(N^2)$ and can become prohibitively expensive at hadron colliders. The first successful application at an $e^+/e^-$ machine was at JADE, and used a distance metric:
%
\begin{equation}
y_{ij} = \frac{2 E_i E_j (1 - \cos \theta_ij)}{Q^2}
\end{equation}
%
where $E$ are the particle energies, $\theta$ is the angle between particles, and $Q$ is the total energy of the event\cite{Jetography}. For massless particles, is just the invariant mass between a pair: the algorithm proceeds to merge objects that have the smallest masses. Jets are merged if $y_\mathrm{min} < y_\mathrm{cut}$, the jet resolution threshold: the number of jets is clearly dependent on the value of this parameter, with more jets produced with a smaller $y_\mathrm{cut}$.

The IRC safety of the JADE algorithm is clear. Soft particles (the worry of infrared safety) are merged together quickly as they have small energies, and colinear particles are merged quickly because of their small angular splitting. The JADE algorithm has a particularly interesting pathology in that soft particles on the opposite sides of the detector can be clustered together if they each have a very small energy: this clearly goes against our intuition that jets should be local groupings, and so improvements became necessary.

The $k_t$ algorithm was the solution--- the only difference is to replace the distance metric with
%
\begin{equation}
y_{ij} = \frac{2 \min(E_i^2,E_j^2) (1-\cos{\theta_{ij}})}{Q^2}
\end{equation}
%


%modifications

%fastjet

%antikt cones

\section{Jet Substructure: Going Deeper}



\section{Calculations with Jets}